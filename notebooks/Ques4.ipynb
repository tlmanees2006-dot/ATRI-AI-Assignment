{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_test, Y_test = load_fashion_mnist()\n",
        "val_size = int(0.1 * X_train.shape[0])\n",
        "X_val, Y_val = X_train[:val_size], Y_train[:val_size]\n",
        "X_train_s, Y_train_s = X_train[val_size:], Y_train[val_size:]"
      ],
      "metadata": {
        "id": "Kn_CInyJiRLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6Ji3h_lf8FK"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"epochs\": {\"values\": [5, 10]},\n",
        "        \"num_layers\": {\"values\": [3, 4, 5]},\n",
        "        \"hidden_size\": {\"values\": [32, 64, 128]},\n",
        "        \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"sgd\", \"momentum\", \"nesterov\", \"rmsprop\", \"adam\", \"nadam\"]\n",
        "        },\n",
        "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sweep_train():\n",
        "    run = wandb.init()\n",
        "    config = wandb.config\n",
        "    hidden_layers = [config.hidden_size] * config.num_layers\n",
        "    model = FNN(\n",
        "        input_size=X_train_s.shape[1],\n",
        "        hidden_layers=hidden_layers,\n",
        "        output_size=10\n",
        "    )\n",
        "    run.name = f\"hl_{config.num_layers}_bs_{config.batch_size}_opt_{config.optimizer}\"\n",
        "    for epoch in range(config.epochs):\n",
        "        perm = np.random.permutation(X_train_s.shape[0])\n",
        "        X_shuf, Y_shuf = X_train_s[perm], Y_train_s[perm]\n",
        "        for i in range(0, X_shuf.shape[0], config.batch_size):\n",
        "            xb = X_shuf[i:i+config.batch_size]\n",
        "            yb = Y_shuf[i:i+config.batch_size]\n",
        "            preds = model.forward(xb)\n",
        "            model.backward(yb)\n",
        "            model.update_parameters(\n",
        "                lr=config.learning_rate,\n",
        "                optimizer=config.optimizer,\n",
        "                t=epoch+1\n",
        "            )\n",
        "        train_preds = model.forward(X_train_s)\n",
        "        train_loss = model.compute_loss(train_preds, Y_train_s)\n",
        "        train_acc = np.mean(\n",
        "        np.argmax(train_preds, axis=1) == np.argmax(Y_train_s, axis=1)\n",
        "        )\n",
        "        val_preds = model.forward(X_val)\n",
        "        val_loss = model.compute_loss(val_preds, Y_val)\n",
        "        val_acc = np.mean(\n",
        "        np.argmax(val_preds, axis=1) == np.argmax(Y_val, axis=1)\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"loss\": train_loss,\n",
        "            \"accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc\n",
        "        })"
      ],
      "metadata": {
        "id": "sgCT4aGCf98l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"fashion-mnist-q4\")\n",
        "wandb.agent(sweep_id, function=sweep_train, count=20)"
      ],
      "metadata": {
        "id": "zb2QKJt1f_ot"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}