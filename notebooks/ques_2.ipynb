{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "y3fKZ8rvFo24"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "def load_fashion_mnist(flatten=True, normalize=True, one_hot=True, num_classes=10):\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    if flatten:\n",
        "        x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "    if normalize:\n",
        "        x_train = x_train / 255.0\n",
        "        x_test = x_test / 255.0\n",
        "    if one_hot:\n",
        "        y_train = np.eye(num_classes)[y_train]\n",
        "        y_test = np.eye(num_classes)[y_test]\n",
        "    return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "def relu_der(z):\n",
        "    return (z > 0).astype(float)\n",
        "def soft(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "VFg9NzwJGI44"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ini_para(layer_sizes):\n",
        "    np.random.seed(42)\n",
        "    params = {}\n",
        "    L = len(layer_sizes)\n",
        "    for l in range(1, L):\n",
        "        params['W'+str(l)] = np.random.randn(layer_sizes[l-1], layer_sizes[l]) * 0.01\n",
        "        params['b'+str(l)] = np.zeros((1, layer_sizes[l]))\n",
        "    return params"
      ],
      "metadata": {
        "id": "r9Xp9oJrgsgX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(X, params):\n",
        "    cache = {'A0': X}\n",
        "    L = len(params)//2\n",
        "    for l in range(1, L):\n",
        "        Z = np.dot(cache['A'+str(l-1)], params['W'+str(l)]) + params['b'+str(l)]\n",
        "        A = relu(Z)\n",
        "        cache['Z'+str(l)] = Z\n",
        "        cache['A'+str(l)] = A\n",
        "    ZL = np.dot(cache['A'+str(L-1)], params['W'+str(L)]) + params['b'+str(L)]\n",
        "    AL = soft(ZL)\n",
        "    cache['Z'+str(L)] = ZL\n",
        "    cache['A'+str(L)] = AL\n",
        "    return AL, cache"
      ],
      "metadata": {
        "id": "cjyMhZnFiBmd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def comp_loss(Y_pred, Y_true):\n",
        "    m = Y_true.shape[0]\n",
        "    loss = -np.sum(Y_true * np.log(Y_pred + 1e-8)) / m\n",
        "    return loss"
      ],
      "metadata": {
        "id": "H9QbZgIWiRhB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(Y_pred, Y_true, params, cache, learning_rate=0.01):\n",
        "    grads = {}\n",
        "    L = len(params)//2\n",
        "    m = Y_true.shape[0]\n",
        "    dZ = Y_pred - Y_true\n",
        "    grads['dW'+str(L)] = np.dot(cache['A'+str(L-1)].T, dZ)/m\n",
        "    grads['db'+str(L)] = np.sum(dZ, axis=0, keepdims=True)/m\n",
        "    dA_prev = np.dot(dZ, params['W'+str(L)].T)\n",
        "    for l in reversed(range(1, L)):\n",
        "        dZ = dA_prev * relu_der(cache['Z'+str(l)])\n",
        "        grads['dW'+str(l)] = np.dot(cache['A'+str(l-1)].T, dZ)/m\n",
        "        grads['db'+str(l)] = np.sum(dZ, axis=0, keepdims=True)/m\n",
        "        if l > 1:\n",
        "            dA_prev = np.dot(dZ, params['W'+str(l)].T)\n",
        "    for l in range(1, L+1):\n",
        "        params['W'+str(l)] -= learning_rate * grads['dW'+str(l)]\n",
        "        params['b'+str(l)] -= learning_rate * grads['db'+str(l)]"
      ],
      "metadata": {
        "id": "_DD2jiIRibVU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(X_train, Y_train, X_test, Y_test, hidden_layers=[128,64], lr=0.01, epochs=20):\n",
        "    layer_sizes = [X_train.shape[1]] + hidden_layers + [10]\n",
        "    params = ini_para(layer_sizes)\n",
        "    for epoch in range(epochs):\n",
        "        Y_pred, cache = forward(X_train, params)\n",
        "        loss = comp_loss(Y_pred, Y_train)\n",
        "        backward(Y_pred, Y_train, params, cache, learning_rate=lr)\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "    Y_pred_test, _ = forward(X_test, params)\n",
        "    accuracy = np.mean(np.argmax(Y_pred_test, axis=1) == np.argmax(Y_test, axis=1))\n",
        "    print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
        "    return params"
      ],
      "metadata": {
        "id": "kQwNKW_ui1hi"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}