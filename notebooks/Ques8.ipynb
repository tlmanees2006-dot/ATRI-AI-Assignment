{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwbKlRnZg-Hx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import wandb\n",
        "wandb.init(\n",
        "    project=\"fashion-mnist-q8_a\",\n",
        "    name=\"CE_vs_Squared_Error\"\n",
        ")\n",
        "from keras.datasets import fashion_mnist\n",
        "def load_fashion_mnist(flatten=True, normalize=True, one_hot=True, num_classes=10):\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    if flatten:\n",
        "        x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "    if normalize:\n",
        "        x_train = x_train / 255.0\n",
        "        x_test = x_test / 255.0\n",
        "    if one_hot:\n",
        "        y_train = np.eye(num_classes)[y_train]\n",
        "        y_test = np.eye(num_classes)[y_test]\n",
        "    return x_train, y_train, x_test, y_test\n",
        "class Activate:\n",
        "    @staticmethod\n",
        "    def relu(Z):\n",
        "        return np.maximum(0, Z)\n",
        "    @staticmethod\n",
        "    def relu_der(Z):\n",
        "        return (Z > 0).astype(float)\n",
        "    @staticmethod\n",
        "    def softmax(Z):\n",
        "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
        "class FNN:\n",
        "    def __init__(self, input_size, hidden_layers, output_size, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.layers = [input_size] + hidden_layers + [output_size]\n",
        "        self.L = len(self.layers) - 1\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        for l in range(1, self.L+1):\n",
        "            limit = np.sqrt(6 / (self.layers[l-1] + self.layers[l]))\n",
        "            self.weights['W'+str(l)] = np.random.uniform(-limit, limit, (self.layers[l-1], self.layers[l]))\n",
        "            self.biases['b'+str(l)] = np.zeros((1, self.layers[l]))\n",
        "        self.opt_caches = {}\n",
        "    def forward(self, X):\n",
        "        self.cache = {'A0': X}\n",
        "        for l in range(1, self.L):\n",
        "            Z = np.dot(self.cache['A'+str(l-1)], self.weights['W'+str(l)]) + self.biases['b'+str(l)]\n",
        "            A = Activate.relu(Z)\n",
        "            self.cache['Z'+str(l)] = Z\n",
        "            self.cache['A'+str(l)] = A\n",
        "        ZL = np.dot(self.cache['A'+str(self.L-1)], self.weights['W'+str(self.L)]) + self.biases['b'+str(self.L)]\n",
        "        AL = Activate.softmax(ZL)\n",
        "        self.cache['Z'+str(self.L)] = ZL\n",
        "        self.cache['A'+str(self.L)] = AL\n",
        "        return AL\n",
        "    def backward(self, Y_true, loss_type=\"cross_entropy\"):\n",
        "        m = Y_true.shape[0]\n",
        "        self.grads = {}\n",
        "        Y_pred = self.cache['A'+str(self.L)]\n",
        "        if loss_type == \"cross_entropy\":\n",
        "          dZ = Y_pred - Y_true\n",
        "        elif loss_type == \"squared_error\":\n",
        "          dZ = (Y_pred - Y_true) * Y_pred * (1 - Y_pred)\n",
        "        self.grads['dW'+str(self.L)] = np.dot(self.cache['A'+str(self.L-1)].T, dZ)/m\n",
        "        self.grads['db'+str(self.L)] = np.sum(dZ, axis=0, keepdims=True)/m\n",
        "        dA_prev = np.dot(dZ, self.weights['W'+str(self.L)].T)\n",
        "        for l in reversed(range(1, self.L)):\n",
        "            dZ = dA_prev * Activate.relu_der(self.cache['Z'+str(l)])\n",
        "            self.grads['dW'+str(l)] = np.dot(self.cache['A'+str(l-1)].T, dZ)/m\n",
        "            self.grads['db'+str(l)] = np.sum(dZ, axis=0, keepdims=True)/m\n",
        "            if l > 1:\n",
        "                dA_prev = np.dot(dZ, self.weights['W'+str(l)].T)\n",
        "    def update_parameters(self, lr=0.01, optimizer='sgd', beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
        "        for l in range(1, self.L+1):\n",
        "            dW = self.grads['dW'+str(l)]\n",
        "            db = self.grads['db'+str(l)]\n",
        "            if optimizer == 'sgd':\n",
        "                self.weights['W'+str(l)] -= lr * dW\n",
        "                self.biases['b'+str(l)] -= lr * db\n",
        "            elif optimizer == 'momentum':\n",
        "                if 'vW'+str(l) not in self.opt_caches:\n",
        "                    self.opt_caches['vW'+str(l)] = np.zeros_like(dW)\n",
        "                    self.opt_caches['vb'+str(l)] = np.zeros_like(db)\n",
        "                self.opt_caches['vW'+str(l)] = beta1*self.opt_caches['vW'+str(l)] + (1-beta1)*dW\n",
        "                self.opt_caches['vb'+str(l)] = beta1*self.opt_caches['vb'+str(l)] + (1-beta1)*db\n",
        "                self.weights['W'+str(l)] -= lr * self.opt_caches['vW'+str(l)]\n",
        "                self.biases['b'+str(l)] -= lr * self.opt_caches['vb'+str(l)]\n",
        "            elif optimizer == 'nesterov':\n",
        "                if 'vW'+str(l) not in self.opt_caches:\n",
        "                    self.opt_caches['vW'+str(l)] = np.zeros_like(dW)\n",
        "                    self.opt_caches['vb'+str(l)] = np.zeros_like(db)\n",
        "                vW_prev = self.opt_caches['vW'+str(l)].copy()\n",
        "                vb_prev = self.opt_caches['vb'+str(l)].copy()\n",
        "                self.opt_caches['vW'+str(l)] = beta1*self.opt_caches['vW'+str(l)] + lr*dW\n",
        "                self.opt_caches['vb'+str(l)] = beta1*self.opt_caches['vb'+str(l)] + lr*db\n",
        "                self.weights['W'+str(l)] -= beta1*vW_prev + (1+beta1)*(self.opt_caches['vW'+str(l)] - vW_prev)\n",
        "                self.biases['b'+str(l)] -= beta1*vb_prev + (1+beta1)*(self.opt_caches['vb'+str(l)] - vb_prev)\n",
        "            elif optimizer == 'rmsprop':\n",
        "                if 'sW'+str(l) not in self.opt_caches:\n",
        "                    self.opt_caches['sW'+str(l)] = np.zeros_like(dW)\n",
        "                    self.opt_caches['sb'+str(l)] = np.zeros_like(db)\n",
        "                self.opt_caches['sW'+str(l)] = beta2*self.opt_caches['sW'+str(l)] + (1-beta2)*(dW**2)\n",
        "                self.opt_caches['sb'+str(l)] = beta2*self.opt_caches['sb'+str(l)] + (1-beta2)*(db**2)\n",
        "                self.weights['W'+str(l)] -= lr * dW / (np.sqrt(self.opt_caches['sW'+str(l)]) + epsilon)\n",
        "                self.biases['b'+str(l)] -= lr * db / (np.sqrt(self.opt_caches['sb'+str(l)]) + epsilon)\n",
        "            elif optimizer == 'adam':\n",
        "                if 'vW'+str(l) not in self.opt_caches:\n",
        "                    self.opt_caches['vW'+str(l)] = np.zeros_like(dW)\n",
        "                    self.opt_caches['vb'+str(l)] = np.zeros_like(db)\n",
        "                    self.opt_caches['sW'+str(l)] = np.zeros_like(dW)\n",
        "                    self.opt_caches['sb'+str(l)] = np.zeros_like(db)\n",
        "                self.opt_caches['vW'+str(l)] = beta1*self.opt_caches['vW'+str(l)] + (1-beta1)*dW\n",
        "                self.opt_caches['vb'+str(l)] = beta1*self.opt_caches['vb'+str(l)] + (1-beta1)*db\n",
        "                self.opt_caches['sW'+str(l)] = beta2*self.opt_caches['sW'+str(l)] + (1-beta2)*(dW**2)\n",
        "                self.opt_caches['sb'+str(l)] = beta2*self.opt_caches['sb'+str(l)] + (1-beta2)*(db**2)\n",
        "                vW_corr = self.opt_caches['vW'+str(l)] / (1 - beta1**t)\n",
        "                vb_corr = self.opt_caches['vb'+str(l)] / (1 - beta1**t)\n",
        "                sW_corr = self.opt_caches['sW'+str(l)] / (1 - beta2**t)\n",
        "                sb_corr = self.opt_caches['sb'+str(l)] / (1 - beta2**t)\n",
        "                self.weights['W'+str(l)] -= lr * vW_corr / (np.sqrt(sW_corr) + epsilon)\n",
        "                self.biases['b'+str(l)] -= lr * vb_corr / (np.sqrt(sb_corr) + epsilon)\n",
        "            elif optimizer == 'nadam':\n",
        "                if 'vW'+str(l) not in self.opt_caches:\n",
        "                    self.opt_caches['vW'+str(l)] = np.zeros_like(dW)\n",
        "                    self.opt_caches['vb'+str(l)] = np.zeros_like(db)\n",
        "                    self.opt_caches['sW'+str(l)] = np.zeros_like(dW)\n",
        "                    self.opt_caches['sb'+str(l)] = np.zeros_like(db)\n",
        "                vW_prev = self.opt_caches['vW'+str(l)].copy()\n",
        "                vb_prev = self.opt_caches['vb'+str(l)].copy()\n",
        "                self.opt_caches['vW'+str(l)] = beta1*self.opt_caches['vW'+str(l)] + (1-beta1)*dW\n",
        "                self.opt_caches['vb'+str(l)] = beta1*self.opt_caches['vb'+str(l)] + (1-beta1)*db\n",
        "                self.opt_caches['sW'+str(l)] = beta2*self.opt_caches['sW'+str(l)] + (1-beta2)*(dW**2)\n",
        "                self.opt_caches['sb'+str(l)] = beta2*self.opt_caches['sb'+str(l)] + (1-beta2)*(db**2)\n",
        "                vW_corr = (beta1*vW_prev + (1+beta1)*self.opt_caches['vW'+str(l)]/(1-beta1**t)) / (1 - beta1**t)\n",
        "                vb_corr = (beta1*vb_prev + (1+beta1)*self.opt_caches['vb'+str(l)]/(1-beta1**t)) / (1 - beta1**t)\n",
        "                sW_corr = self.opt_caches['sW'+str(l)] / (1 - beta2**t)\n",
        "                sb_corr = self.opt_caches['sb'+str(l)] / (1 - beta2**t)\n",
        "                self.weights['W'+str(l)] -= lr * vW_corr / (np.sqrt(sW_corr) + epsilon)\n",
        "                self.biases['b'+str(l)] -= lr * vb_corr / (np.sqrt(sb_corr) + epsilon)\n",
        "    def cross_entropy_loss(self, Y_pred, Y_true):\n",
        "        m = Y_true.shape[0]\n",
        "        return -np.sum(Y_true*np.log(Y_pred + 1e-8))/m\n",
        "    def squared_error_loss(self, Y_pred, Y_true):\n",
        "        return np.mean(np.sum((Y_pred - Y_true) ** 2, axis=1))\n",
        "    def compute_loss(self, Y_pred, Y_true):\n",
        "        return self.cross_entropy_loss(Y_pred, Y_true)\n",
        "    def train(self, X_train, Y_train, X_test, Y_test, epochs=10, batch_size=64, lr=0.01, optimizer='sgd'):\n",
        "        num_samples = X_train.shape[0]\n",
        "        loss_history_ce = []\n",
        "        loss_history_se = []\n",
        "        for epoch in range(epochs):\n",
        "          perm = np.random.permutation(num_samples)\n",
        "          X_shuf, Y_shuf = X_train[perm], Y_train[perm]\n",
        "          epoch_ce, epoch_se, num_batches = 0, 0, 0\n",
        "          for i in range(0, num_samples, batch_size):\n",
        "            Xb = X_shuf[i:i+batch_size]\n",
        "            Yb = Y_shuf[i:i+batch_size]\n",
        "            Y_pred = self.forward(Xb)\n",
        "            loss_ce = self.cross_entropy_loss(Y_pred, Yb)\n",
        "            loss_se = self.squared_error_loss(Y_pred, Yb)\n",
        "            self.backward(Yb, loss_type=\"cross_entropy\")\n",
        "            self.update_parameters(lr=lr, optimizer=optimizer, t=epoch+1)\n",
        "            epoch_ce += loss_ce\n",
        "            epoch_se += loss_se\n",
        "            num_batches += 1\n",
        "          epoch_ce /= num_batches\n",
        "          epoch_se /= num_batches\n",
        "          loss_history_ce.append(epoch_ce)\n",
        "          loss_history_se.append(epoch_se)\n",
        "          Y_test_pred = self.forward(X_test)\n",
        "          test_acc = np.mean(\n",
        "          np.argmax(Y_test_pred, axis=1) == np.argmax(Y_test, axis=1)\n",
        "          )\n",
        "          wandb.log({\n",
        "              \"epoch\": epoch,\n",
        "              \"cross_entropy_loss\": epoch_ce,\n",
        "              \"squared_error_loss\": epoch_se,\n",
        "              \"test_accuracy\": test_acc\n",
        "          })\n",
        "        wandb.log({\n",
        "                \"loss_comparison\": wandb.plot.line_series(\n",
        "                xs=list(range(len(loss_history_ce))),\n",
        "                ys=[loss_history_ce, loss_history_se],\n",
        "                keys=[\"Cross Entropy\", \"Squared Error\"],\n",
        "                title=\"Loss Comparison: Cross-Entropy vs Squared Error\",\n",
        "                xname=\"Epoch\"\n",
        "                )\n",
        "                })"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train, X_test, Y_test = load_fashion_mnist()\n",
        "model = FNN(\n",
        "    input_size=784,\n",
        "    hidden_layers=[128, 64],\n",
        "    output_size=10\n",
        ")\n",
        "model.train(\n",
        "    X_train, Y_train,\n",
        "    X_test, Y_test,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    lr=0.01,\n",
        "    optimizer='adam'\n",
        ")\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "nbJYW5lxhFQC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}